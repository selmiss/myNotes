---
typora-copy-images-to: ./神经网络学习笔记pic
typora-root-url: ./
---

# 神经网络学习笔记

## 一、基础知识

### 1.1 K近邻算法（KNN）

K：近邻计算流程：

1. 计算已知类别数据集中的点与当前点的距离
2. 按照距离依次排序
3. 选取与当前点距离最小的K个点
4. 确定前K个点所在类别的出现概率
5. 返回前K个点出现频率最高的类别作为当前点预测分类

### 1.2 得分函数

$ f(x,W) = W*x+b$

<img src="./神经网络学习笔记pic/image-20221227212705187.png" alt="image-20221227212705187" style="zoom:50%;" />

### 1.3 损失函数

> 第i个类别的损失函数，其他类别的得分$s_j$减去正确类别的$s_y$的得分+1

<img src="./神经网络学习笔记pic/image-20221227212530969.png" alt="image-20221227212530969" style="zoom:50%;" />

<img src="./神经网络学习笔记pic/image-20221227213129024.png" alt="image-20221227213129024" style="zoom:30%;" />

> 正则化

正则化惩罚：

<img src="./神经网络学习笔记pic/image-20221227213157541.png" alt="image-20221227213157541" style="zoom:50%;" />

正则化惩罚项：

<img src="./神经网络学习笔记pic/image-20221227213245403.png" alt="image-20221227213245403" style="zoom:50%;" />

### 1.4 Softmax分类器

> 归一化处理

其中s为得分函数

<img src="./神经网络学习笔记pic/image-20221227213757184.png" alt="image-20221227213757184" style="zoom:50%;" />

<img src="./神经网络学习笔记pic/image-20221227213808253.png" alt="image-20221227213808253" style="zoom:50%;" />

<img src="./神经网络学习笔记pic//Users/fancy/Library/Application Support/typora-user-images/image-20221227213656638.png" alt="image-20221227213656638" style="zoom:50%;" />

计算损失值：

<img src="./神经网络学习笔记pic/image-20221227213837241.png" alt="image-20221227213837241" style="zoom:50%;" />

概率为0到1，取对数函数，损失为0到+∞

### 1.5 前向传播

计算得出损失值，交给反向传播

<img src="./神经网络学习笔记pic/image-20221227222216448.png" alt="image-20221227222216448" style="zoom:50%;" />

### 1.6 梯度下降

### 1.7 反向传播

通过反向偏导计算得到输入变量梯度的变化

- 加法门单元：均等分配

- max门单元：给最大的分配

- 乘法门单元：互换梯度

<img src="./神经网络学习笔记pic/image-20221227223017291.png" alt="image-20221227223017291" style="zoom:50%;" />

### 1.8 层次结构

<img src="./神经网络学习笔记pic/image-20221227223140960.png" alt="image-20221227223140960" style="zoom:50%;" />

### 1.9 激活函数

Sigmoid，Relu，Tanh等

<img src="./神经网络学习笔记pic/image-20221227223906589.png" alt="image-20221227223906589" style="zoom:50%;" />

<img src="./神经网络学习笔记pic/image-20221227223916080.png" alt="image-20221227223916080" style="zoom:50%;" />

### 1.10 参数初始化

![image-20221228114455012](./神经网络学习笔记pic/image-20221228114455012-2199100-2199103.png)

### 1.11 DROP-OUT

杀死一部分神经元，防止网络模型过于复杂。

## 二、卷积神经网络

### 2.1 卷积网络

![image-20221228160934360](./神经网络学习笔记pic/image-20221228160934360.png)

> 整体架构

- 输入层
- 卷积层
- 池化层
- 全连接层

> 卷积计算原理

输入的矩阵为$7*7*3$

权重矩阵，3*3为卷积核尺寸，深度为3应当与输入矩阵深度保持一致

$W_i$的个数为卷积核个数

<img src="./神经网络学习笔记pic/image-20221228162138264.png" alt="image-20221228162138264" style="zoom:50%;" />

> 卷积层涉及参数

- 步长 （图像通常为1）
- 卷积核尺寸 （最小通常为3*3）
- 边缘填充 （+pad 1）：弥补边缘信息被利用少的问题，以0为填充，不影响最后结果
- 卷积核个数

### 2.2 卷积结果计算公式

- 长度：

  <img src="./神经网络学习笔记pic/image-20221228163211094.png" alt="image-20221228163211094" style="zoom:100%;" />

- 宽度：

  ![image-20221228163232642](./神经网络学习笔记pic/image-20221228163232642.png)

  其中W1、H1表示输入的宽度、长度；W2、H2表示输出特征图的宽度、长度； 

  F表示卷积核长和宽的大小；S表示滑动窗口的步长;P表示边界填充(加几圈0)。

### 2.3 卷积参数共享

每一个filter的值在窗口滑动时不变，共享。

### 2.4 池化层

在原始特征上进行筛选，压缩，没有涉及到矩阵计算

> 最大池化：选择最大的值
>
> 平均池化：计算平均值（少见）

<img src="./神经网络学习笔记pic/image-20221228163758786.png" alt="image-20221228163758786" style="zoom:50%;" />

