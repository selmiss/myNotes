---
typora-copy-images-to: ./神经网络学习笔记pic
typora-root-url: ./
---

# 神经网络学习笔记

## 一、基础知识

### 1.1 K近邻算法（KNN）

K：近邻计算流程：

1. 计算已知类别数据集中的点与当前点的距离
2. 按照距离依次排序
3. 选取与当前点距离最小的K个点
4. 确定前K个点所在类别的出现概率
5. 返回前K个点出现频率最高的类别作为当前点预测分类

### 1.2 得分函数

$ f(x,W) = W*x+b$

<img src="./神经网络学习笔记pic/image-20221227212705187.png" alt="image-20221227212705187" style="zoom:50%;" />

### 1.3 损失函数

> 第i个类别的损失函数，其他类别的得分$s_j$减去正确类别的$s_y$的得分+1

<img src="./神经网络学习笔记pic/image-20221227212530969.png" alt="image-20221227212530969" style="zoom:50%;" />

<img src="./神经网络学习笔记pic/image-20221227213129024.png" alt="image-20221227213129024" style="zoom:30%;" />

> 正则化

正则化惩罚：

<img src="./神经网络学习笔记pic/image-20221227213157541.png" alt="image-20221227213157541" style="zoom:50%;" />

正则化惩罚项：

<img src="./神经网络学习笔记pic/image-20221227213245403.png" alt="image-20221227213245403" style="zoom:50%;" />

### 1.4 Softmax分类器

> 归一化处理

其中s为得分函数

<img src="./神经网络学习笔记pic/image-20221227213757184.png" alt="image-20221227213757184" style="zoom:50%;" />

<img src="./神经网络学习笔记pic/image-20221227213808253.png" alt="image-20221227213808253" style="zoom:50%;" />

<img src="./神经网络学习笔记pic/image-20221228175232141.png" alt="image-20221228175232141" style="zoom:50%;" />

计算损失值：

<img src="./神经网络学习笔记pic/image-20221227213837241.png" alt="image-20221227213837241" style="zoom:50%;" />

概率为0到1，取对数函数，损失为0到+∞

### 1.5 前向传播

计算得出损失值，交给反向传播

<img src="./神经网络学习笔记pic/image-20221227222216448.png" alt="image-20221227222216448" style="zoom:50%;" />

### 1.6 梯度下降

### 1.7 反向传播

通过反向偏导计算得到输入变量梯度的变化

- 加法门单元：均等分配

- max门单元：给最大的分配

- 乘法门单元：互换梯度

<img src="./神经网络学习笔记pic/image-20221227223017291.png" alt="image-20221227223017291" style="zoom:50%;" />

### 1.8 层次结构

<img src="./神经网络学习笔记pic/image-20221227223140960.png" alt="image-20221227223140960" style="zoom:50%;" />

### 1.9 激活函数

Sigmoid，Relu，Tanh等

<img src="./神经网络学习笔记pic/image-20221227223906589.png" alt="image-20221227223906589" style="zoom:50%;" />

<img src="./神经网络学习笔记pic/image-20221227223916080.png" alt="image-20221227223916080" style="zoom:50%;" />

### 1.10 参数初始化

![image-20221228114455012](./神经网络学习笔记pic/image-20221228114455012-2199100-2199103.png)

### 1.11 DROP-OUT

杀死一部分神经元，防止网络模型过于复杂。

## 二、卷积神经网络

特征图变化：

![image-20221228164555265](./神经网络学习笔记pic/image-20221228164555265.png)

### 2.1 卷积网络

![image-20221228160934360](./神经网络学习笔记pic/image-20221228160934360.png)

> 整体架构

- 输入层
- 卷积层
- 池化层
- 全连接层

> 卷积计算原理

输入的矩阵为$7*7*3$

权重矩阵，3*3为卷积核尺寸，深度为3应当与输入矩阵深度保持一致

$W_i$的个数为卷积核个数

<img src="./神经网络学习笔记pic/image-20221228162138264.png" alt="image-20221228162138264" style="zoom:50%;" />

> 卷积层涉及参数

- 步长 （图像通常为1）
- 卷积核尺寸 （最小通常为3*3）
- 边缘填充 （+pad 1）：弥补边缘信息被利用少的问题，以0为填充，不影响最后结果
- 卷积核个数

### 2.2 卷积结果计算公式

- 长度：

  <img src="./神经网络学习笔记pic/image-20221228163211094.png" alt="image-20221228163211094" style="zoom:100%;" />

- 宽度：

  ![image-20221228163232642](./神经网络学习笔记pic/image-20221228163232642.png)

  其中W1、H1表示输入的宽度、长度；W2、H2表示输出特征图的宽度、长度； 

  F表示卷积核长和宽的大小；S表示滑动窗口的步长;P表示边界填充(加几圈0)。

### 2.3 卷积参数共享

每一个filter的值在窗口滑动时不变，共享。

### 2.4 池化层

在原始特征上进行筛选，压缩，没有涉及到矩阵计算

> 最大池化：选择最大的值
>
> 平均池化：计算平均值（少见）

<img src="./神经网络学习笔记pic/image-20221228163758786.png" alt="image-20221228163758786" style="zoom:50%;" />

### 2.5 一些经典网络

- ALexnet

- Vgg

  用小的卷积核来完成特征提取操作

  <img src="./神经网络学习笔记pic/image-20221228164808577.png" alt="image-20221228164808577" style="zoom:80%;" />

- Resnet（残差网络）

  该网络被证明效果比较好，主流网络。

  层数的增多使得错误率上升的问题，去除其中表现不好的层

  <img src="./神经网络学习笔记pic/image-20221228165259860.png" alt="image-20221228165259860" style="zoom:50%;" />

### 2.6 感受野

当前层的一个数据是由之前多少个数据参与计算得到的。

下图感受野分别为1，3\*3，5\*5

<img src="./神经网络学习笔记pic/image-20221228165806910.png" alt="image-20221228165806910" style="zoom:50%;" />

## 三、RNN/LSTM-循环神经网络

> RNN网络结构：

<img src="./神经网络学习笔记pic/image-20221228170449771.png" alt="image-20221228170449771" style="zoom:80%;" />

递归结构，前一个A作为后一个A的输入，x为时间序列输入，最后取$h_t$来作为最后结果。考虑了之前全部结果。而LSTM能够选择性忘记一些特征。

<img src="./神经网络学习笔记pic/image-20221228170554789.png" alt="image-20221228170554789" style="zoom:80%;" />

> LSTM网络结构

![image-20221228170835571](./神经网络学习笔记pic/image-20221228170835571.png)

其中C为控制参数，决定什么样的信息会被保留。

## 四、NLP-Word2Vec

文本向量化

50-300维的向量

Word2Vec将词转换为向量

相近词的向量的空间位置接近

滑动窗口构建训练数据集

### 不同模型对比

- 输入上下文，输出词

<img src="./神经网络学习笔记pic/image-20221228173443770.png" alt="image-20221228173443770" style="zoom:50%;" />

- 输入词，输出上下文匹配词

<img src="./神经网络学习笔记pic/image-20221228173533182.png" alt="image-20221228173533182" style="zoom:50%;" />

如果语料库过大，输入两个词，输出概率。需要添加概率为0的数据【负采样模型】。

变成一个二分类模型。负采样5个。