---
typora-copy-images-to: ./神经网络学习笔记pic
typora-root-url: ./神经网络学习笔记pic
---

# 神经网络学习笔记

## 一、基础知识

### 1.1 K近邻算法（KNN）

K：近邻计算流程：

1. 计算已知类别数据集中的点与当前点的距离
2. 按照距离依次排序
3. 选取与当前点距离最小的K个点
4. 确定前K个点所在类别的出现概率
5. 返回前K个点出现频率最高的类别作为当前点预测分类

### 1.2 得分函数

$ f(x,W) = W*x+b$

<img src="image-20221227212705187.png" alt="image-20221227212705187" style="zoom:50%;" />

### 1.3 损失函数

> 第i个类别的损失函数，其他类别的得分$s_j$减去正确类别的$s_y$的得分+1

<img src="image-20221227212530969.png" alt="image-20221227212530969" style="zoom:50%;" />

<img src="image-20221227213129024.png" alt="image-20221227213129024" style="zoom:30%;" />

> 正则化

正则化惩罚：

<img src="image-20221227213157541.png" alt="image-20221227213157541" style="zoom:50%;" />

正则化惩罚项：

<img src="image-20221227213245403.png" alt="image-20221227213245403" style="zoom:50%;" />

### 1.4 Softmax分类器

> 归一化处理

其中s为得分函数

<img src="image-20221227213757184.png" alt="image-20221227213757184" style="zoom:50%;" />

<img src="image-20221227213808253.png" alt="image-20221227213808253" style="zoom:50%;" />

<img src="/Users/fancy/Library/Application Support/typora-user-images/image-20221227213656638.png" alt="image-20221227213656638" style="zoom:50%;" />

计算损失值：

<img src="image-20221227213837241.png" alt="image-20221227213837241" style="zoom:50%;" />

概率为0到1，取对数函数，损失为0到+∞

### 1.5 前向传播

计算得出损失值，交给反向传播

<img src="image-20221227222216448.png" alt="image-20221227222216448" style="zoom:50%;" />

### 1.6 梯度下降

### 1.7 反向传播

通过反向偏导计算得到输入变量梯度的变化

- 加法门单元：均等分配

- max门单元：给最大的分配

- 乘法门单元：互换梯度

<img src="image-20221227223017291.png" alt="image-20221227223017291" style="zoom:50%;" />

### 1.8 层次结构

<img src="image-20221227223140960.png" alt="image-20221227223140960" style="zoom:50%;" />

### 1.9 激活函数

Sigmoid，Relu，Tanh等

<img src="image-20221227223906589.png" alt="image-20221227223906589" style="zoom:50%;" />

<img src="image-20221227223916080.png" alt="image-20221227223916080" style="zoom:50%;" />

### 1.10 参数初始化

![image-20221228114455012](/image-20221228114455012-2199100-2199103.png)

### 1.11 DROP-OUT

杀死一部分神经元，防止网络模型过于复杂。